---
title: 多特性
tags:  [特征缩放, 归一化, 机器学习]
layout: post
description: 
comments: true
published: true
mathjax: true
categories: Machine-Learning
date: 2017-10-19 20:52:00 +0900
---

## 多特性模型

[![](/assets/images/ML-4-1-2017-10-19-20-52-46.png)](https://www.bilibili.com/video/av9912938/index_13.html#page=19)

当有n个特征时，线性模型就变成了如下的形式：

$$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n$$

如果令$$x_0=1$$，上述公式可以写成：

$$h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n$$

设：

$$
X=\begin{pmatrix}
x_0 \\ x_1 \\ x_2 \\ ... \\ x_n
\end{pmatrix}\in\mathbb{R}^{n+1}
 And
\Theta=\begin{pmatrix}x_0\\x_1\\x_2\\...\\x_n\end{pmatrix}\in\mathbb{R}^{n+1}
$$

则n个特征的线性模型公式就可以简化写为：

$$
h_\theta(x)=\Theta^TX
$$

## 多特征梯度下降

[![](/assets/images/ML-4-2-2017-10-20-07-48-07.png)](https://www.bilibili.com/video/av9912938/index_13.html#page=20)

特征数量n>1时和n=1时，代价函数是一样的，因此可以使用特征数量为1时的处理方式去处理多特征的情况。

## 特征缩放

[![](/assets/images/ML-4-3-2017-10-20-07-37-23.png)](https://www.bilibili.com/video/av9912938/index_20.html#page=21)

把所有特征变化范围变化为$$-1\leq x_i \leq 1$$（特征归一化），能让梯度下降算法更快的收敛，从而更快的找到最优解。

## 学习率的选择

[![](/assets/images/ML-4-4-2017-10-20-07-59-49.png)](https://www.bilibili.com/video/av9912938/index_20.html#page=22)

选择学习率比较好的办法是观察收敛图，寻找能让收敛图是向下倾斜曲线的学习率。

![](/assets/images/ML-4-4-2017-10-20-08-00-48.png)

如果学习率太大会导致无法收敛，即Overfit。学习率只要足够小，都能保证收敛，但唯一的问题是需要过多的迭代计算。

## 特征和多项式回归

[![](/assets/images/ML-4-5-2017-10-21-08-44-01.png)](https://www.bilibili.com/video/av9912938/index_22.html#page=23)

有时候为了更好的拟合数据，需要选择更合适的函数模型，比如：3次函数模型，而不是固定使用一次线性函数模型。于是，我们必须要对特征先做变换处理，比如：把原来的两个特征房屋的临街宽度和房屋长度相乘结果作为一个特征。

![变换特征](/assets/images/ML-4-5-2017-10-21-08-50-50.png)

